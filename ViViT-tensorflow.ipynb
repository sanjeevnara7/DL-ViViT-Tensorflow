{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjeevnara/miniforge3/envs/tf_Dev/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import medmnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Video Data\n",
    "**Organ MNIST 3D (Medical Video Data for Classification)** <br/>\n",
    "Video Data Shape: 28 frames x (28 x 28 x 1) per frame <br/>\n",
    "Number of Classes: 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PARAMETERS\n",
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (28, 28, 28, 1)\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# Utility Function to get data\n",
    "def download_and_prepare_dataset(data_info):\n",
    "    \"\"\"Utility function to download the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata.\n",
    "    \"\"\"\n",
    "    data_path = tf.keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
    "\n",
    "    # Use numpy to load data \n",
    "    with np.load(data_path) as data:\n",
    "        # Get videos\n",
    "        train_videos = data[\"train_images\"]\n",
    "        valid_videos = data[\"val_images\"]\n",
    "        test_videos = data[\"test_images\"]\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data[\"train_labels\"].flatten()\n",
    "        valid_labels = data[\"val_labels\"].flatten()\n",
    "        test_labels = data[\"test_labels\"].flatten()\n",
    "\n",
    "    return (\n",
    "        (train_videos, train_labels),\n",
    "        (valid_videos, valid_labels),\n",
    "        (test_videos, test_labels),\n",
    "    )\n",
    "\n",
    "# Get the metadata of the dataset\n",
    "info = medmnist.INFO[DATASET_NAME]\n",
    "\n",
    "# Get the dataset\n",
    "prepared_dataset = download_and_prepare_dataset(info)\n",
    "(train_videos, train_labels) = prepared_dataset[0]\n",
    "(valid_videos, valid_labels) = prepared_dataset[1]\n",
    "(test_videos, test_labels) = prepared_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 28) <class 'numpy.ndarray'>\n",
      "() <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "print(train_videos[0].shape, type(train_videos[0]))\n",
    "print(train_labels[0].shape, type(train_labels[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.Data pipeline for the model\n",
    "\n",
    "Prepare the Dataloaders for training. We use a batch size of 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 15:59:33.373729: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-28 15:59:33.373875: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 28, 28, 28, 1)\n",
      "(32,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 15:59:33.481379: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "for frames,label in trainloader.take(1):\n",
    "    print(frames.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Compile the model\n",
    "\n",
    "We first implement all the necessary layer blocks of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tublet Embedding Layer to extract volume patches from the video and convert to embeddings\n",
    "class TubeletEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = tf.keras.layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n",
    "\n",
    "# Positional Encoder to add positional information to embeddings\n",
    "class PositionalEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8) # 3D patches (volumes)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2 # We assume square input\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128 # Embedding Dim\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8\n",
    "\n",
    "# Helper function to initialize the model\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block:\n",
    "    # input\n",
    "    # |      \\ \n",
    "    # |       \\\n",
    "    # MSA     |\n",
    "    # |       /\n",
    "    # Skip(+)\n",
    "    # |       \\\n",
    "    # |        \\\n",
    "    # LayerNorm \\\n",
    "    # |          |\n",
    "    # GeLU MLP(embed_dim x 4, embed_dim)\n",
    "    # |          |\n",
    "    # |        /\n",
    "    # |       /\n",
    "    # Skip(+) \n",
    "    # |\n",
    "    # Output  \n",
    "\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = tf.keras.layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                tf.keras.layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = tf.keras.layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = tf.keras.layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = tf.keras.layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 16:00:41.184253: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - ETA: 0s - loss: 2.4913 - accuracy: 0.1070 - top-5-accuracy: 0.5576"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 16:00:53.625736: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 19s 381ms/step - loss: 2.4913 - accuracy: 0.1070 - top-5-accuracy: 0.5576 - val_loss: 2.4382 - val_accuracy: 0.0870 - val_top-5-accuracy: 0.5776\n",
      "Epoch 2/60\n",
      "31/31 [==============================] - 6s 202ms/step - loss: 2.2656 - accuracy: 0.1543 - top-5-accuracy: 0.6626 - val_loss: 2.2604 - val_accuracy: 0.2484 - val_top-5-accuracy: 0.6832\n",
      "Epoch 3/60\n",
      "31/31 [==============================] - 6s 201ms/step - loss: 2.0993 - accuracy: 0.2263 - top-5-accuracy: 0.7603 - val_loss: 1.8563 - val_accuracy: 0.2857 - val_top-5-accuracy: 0.8882\n",
      "Epoch 4/60\n",
      "31/31 [==============================] - 6s 207ms/step - loss: 1.8830 - accuracy: 0.2757 - top-5-accuracy: 0.8519 - val_loss: 1.5766 - val_accuracy: 0.3106 - val_top-5-accuracy: 0.8944\n",
      "Epoch 5/60\n",
      "31/31 [==============================] - 6s 205ms/step - loss: 1.6511 - accuracy: 0.3693 - top-5-accuracy: 0.8971 - val_loss: 1.3644 - val_accuracy: 0.4161 - val_top-5-accuracy: 0.9441\n",
      "Epoch 6/60\n",
      "31/31 [==============================] - 7s 223ms/step - loss: 1.4759 - accuracy: 0.4126 - top-5-accuracy: 0.9321 - val_loss: 1.1499 - val_accuracy: 0.4969 - val_top-5-accuracy: 0.9814\n",
      "Epoch 7/60\n",
      "31/31 [==============================] - 7s 224ms/step - loss: 1.3468 - accuracy: 0.4866 - top-5-accuracy: 0.9465 - val_loss: 1.2214 - val_accuracy: 0.5155 - val_top-5-accuracy: 0.9689\n",
      "Epoch 8/60\n",
      "31/31 [==============================] - 7s 220ms/step - loss: 1.3109 - accuracy: 0.4949 - top-5-accuracy: 0.9434 - val_loss: 1.1639 - val_accuracy: 0.5155 - val_top-5-accuracy: 0.9938\n",
      "Epoch 9/60\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 1.2669 - accuracy: 0.5309 - top-5-accuracy: 0.9465 - val_loss: 0.9265 - val_accuracy: 0.6087 - val_top-5-accuracy: 0.9938\n",
      "Epoch 10/60\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 1.0832 - accuracy: 0.5998 - top-5-accuracy: 0.9630 - val_loss: 1.0644 - val_accuracy: 0.5714 - val_top-5-accuracy: 0.9814\n",
      "Epoch 11/60\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 0.9936 - accuracy: 0.6183 - top-5-accuracy: 0.9794 - val_loss: 0.8787 - val_accuracy: 0.6335 - val_top-5-accuracy: 0.9814\n",
      "Epoch 12/60\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 1.0024 - accuracy: 0.6060 - top-5-accuracy: 0.9763 - val_loss: 0.8084 - val_accuracy: 0.7143 - val_top-5-accuracy: 0.9752\n",
      "Epoch 13/60\n",
      "31/31 [==============================] - 7s 232ms/step - loss: 0.9200 - accuracy: 0.6420 - top-5-accuracy: 0.9794 - val_loss: 0.8673 - val_accuracy: 0.6273 - val_top-5-accuracy: 0.9689\n",
      "Epoch 14/60\n",
      "31/31 [==============================] - 7s 226ms/step - loss: 0.8376 - accuracy: 0.6811 - top-5-accuracy: 0.9866 - val_loss: 0.6511 - val_accuracy: 0.7391 - val_top-5-accuracy: 0.9938\n",
      "Epoch 15/60\n",
      "31/31 [==============================] - 7s 212ms/step - loss: 0.8150 - accuracy: 0.6811 - top-5-accuracy: 0.9856 - val_loss: 0.7002 - val_accuracy: 0.7826 - val_top-5-accuracy: 0.9814\n",
      "Epoch 16/60\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 0.7679 - accuracy: 0.7037 - top-5-accuracy: 0.9897 - val_loss: 0.8367 - val_accuracy: 0.7267 - val_top-5-accuracy: 0.9752\n",
      "Epoch 17/60\n",
      "31/31 [==============================] - 6s 205ms/step - loss: 0.7305 - accuracy: 0.7212 - top-5-accuracy: 0.9897 - val_loss: 0.5877 - val_accuracy: 0.7950 - val_top-5-accuracy: 0.9938\n",
      "Epoch 18/60\n",
      "31/31 [==============================] - 6s 210ms/step - loss: 0.6631 - accuracy: 0.7593 - top-5-accuracy: 0.9928 - val_loss: 0.5436 - val_accuracy: 0.8447 - val_top-5-accuracy: 0.9938\n",
      "Epoch 19/60\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 0.6120 - accuracy: 0.7767 - top-5-accuracy: 0.9969 - val_loss: 0.4709 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
      "Epoch 20/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.5449 - accuracy: 0.8056 - top-5-accuracy: 0.9949 - val_loss: 0.6498 - val_accuracy: 0.7888 - val_top-5-accuracy: 0.9814\n",
      "Epoch 21/60\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 0.5705 - accuracy: 0.7850 - top-5-accuracy: 0.9928 - val_loss: 0.5582 - val_accuracy: 0.7826 - val_top-5-accuracy: 0.9938\n",
      "Epoch 22/60\n",
      "31/31 [==============================] - 6s 210ms/step - loss: 0.5103 - accuracy: 0.8158 - top-5-accuracy: 0.9990 - val_loss: 0.5104 - val_accuracy: 0.8447 - val_top-5-accuracy: 0.9938\n",
      "Epoch 23/60\n",
      "31/31 [==============================] - 7s 211ms/step - loss: 0.4297 - accuracy: 0.8549 - top-5-accuracy: 0.9969 - val_loss: 0.5118 - val_accuracy: 0.8509 - val_top-5-accuracy: 0.9876\n",
      "Epoch 24/60\n",
      "31/31 [==============================] - 7s 211ms/step - loss: 0.4234 - accuracy: 0.8611 - top-5-accuracy: 0.9979 - val_loss: 0.4796 - val_accuracy: 0.8261 - val_top-5-accuracy: 0.9938\n",
      "Epoch 25/60\n",
      "31/31 [==============================] - 6s 210ms/step - loss: 0.3982 - accuracy: 0.8591 - top-5-accuracy: 1.0000 - val_loss: 0.5738 - val_accuracy: 0.8447 - val_top-5-accuracy: 0.9876\n",
      "Epoch 26/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.5149 - accuracy: 0.8056 - top-5-accuracy: 0.9949 - val_loss: 0.4695 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9938\n",
      "Epoch 27/60\n",
      "31/31 [==============================] - 6s 202ms/step - loss: 0.3428 - accuracy: 0.8848 - top-5-accuracy: 0.9990 - val_loss: 0.3764 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
      "Epoch 28/60\n",
      "31/31 [==============================] - 6s 203ms/step - loss: 0.3344 - accuracy: 0.8848 - top-5-accuracy: 0.9990 - val_loss: 0.3705 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9876\n",
      "Epoch 29/60\n",
      "31/31 [==============================] - 6s 208ms/step - loss: 0.2621 - accuracy: 0.9053 - top-5-accuracy: 0.9990 - val_loss: 0.4009 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9876\n",
      "Epoch 30/60\n",
      "31/31 [==============================] - 7s 221ms/step - loss: 0.2940 - accuracy: 0.8951 - top-5-accuracy: 1.0000 - val_loss: 0.4220 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9938\n",
      "Epoch 31/60\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 0.2366 - accuracy: 0.9280 - top-5-accuracy: 0.9979 - val_loss: 0.4166 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 32/60\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 0.2070 - accuracy: 0.9228 - top-5-accuracy: 1.0000 - val_loss: 0.3571 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.2614 - accuracy: 0.9095 - top-5-accuracy: 0.9990 - val_loss: 0.4011 - val_accuracy: 0.8820 - val_top-5-accuracy: 0.9938\n",
      "Epoch 34/60\n",
      "31/31 [==============================] - 6s 208ms/step - loss: 0.1761 - accuracy: 0.9403 - top-5-accuracy: 1.0000 - val_loss: 0.3582 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9876\n",
      "Epoch 35/60\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 0.1617 - accuracy: 0.9465 - top-5-accuracy: 1.0000 - val_loss: 0.5004 - val_accuracy: 0.8385 - val_top-5-accuracy: 0.9938\n",
      "Epoch 36/60\n",
      "31/31 [==============================] - 7s 230ms/step - loss: 0.1843 - accuracy: 0.9403 - top-5-accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
      "Epoch 37/60\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 0.1210 - accuracy: 0.9650 - top-5-accuracy: 1.0000 - val_loss: 0.2857 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9876\n",
      "Epoch 38/60\n",
      "31/31 [==============================] - 6s 200ms/step - loss: 0.0998 - accuracy: 0.9691 - top-5-accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 39/60\n",
      "31/31 [==============================] - 6s 202ms/step - loss: 0.1201 - accuracy: 0.9660 - top-5-accuracy: 1.0000 - val_loss: 0.3235 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9876\n",
      "Epoch 40/60\n",
      "31/31 [==============================] - 6s 208ms/step - loss: 0.0890 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9938\n",
      "Epoch 41/60\n",
      "31/31 [==============================] - 7s 216ms/step - loss: 0.1265 - accuracy: 0.9537 - top-5-accuracy: 1.0000 - val_loss: 0.2595 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 42/60\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 0.1174 - accuracy: 0.9558 - top-5-accuracy: 1.0000 - val_loss: 0.4295 - val_accuracy: 0.8696 - val_top-5-accuracy: 0.9876\n",
      "Epoch 43/60\n",
      "31/31 [==============================] - 7s 218ms/step - loss: 0.0975 - accuracy: 0.9722 - top-5-accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 44/60\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 0.0709 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.3310 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 45/60\n",
      "31/31 [==============================] - 7s 217ms/step - loss: 0.0780 - accuracy: 0.9763 - top-5-accuracy: 1.0000 - val_loss: 0.3157 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9876\n",
      "Epoch 46/60\n",
      "31/31 [==============================] - 7s 214ms/step - loss: 0.1194 - accuracy: 0.9578 - top-5-accuracy: 1.0000 - val_loss: 0.4103 - val_accuracy: 0.8882 - val_top-5-accuracy: 0.9876\n",
      "Epoch 47/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.0589 - accuracy: 0.9856 - top-5-accuracy: 1.0000 - val_loss: 0.3226 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 48/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.0505 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.3754 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9876\n",
      "Epoch 49/60\n",
      "31/31 [==============================] - 7s 211ms/step - loss: 0.0362 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.2978 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 50/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.0663 - accuracy: 0.9815 - top-5-accuracy: 1.0000 - val_loss: 0.5077 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9876\n",
      "Epoch 51/60\n",
      "31/31 [==============================] - 6s 208ms/step - loss: 0.0551 - accuracy: 0.9774 - top-5-accuracy: 1.0000 - val_loss: 0.4294 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9876\n",
      "Epoch 52/60\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 0.0484 - accuracy: 0.9877 - top-5-accuracy: 1.0000 - val_loss: 0.2876 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 53/60\n",
      "31/31 [==============================] - 6s 206ms/step - loss: 0.0173 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 54/60\n",
      "31/31 [==============================] - 7s 212ms/step - loss: 0.0146 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.2918 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 55/60\n",
      "31/31 [==============================] - 7s 215ms/step - loss: 0.0287 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.2798 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 56/60\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 0.0265 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.2780 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9876\n",
      "Epoch 57/60\n",
      "31/31 [==============================] - 7s 218ms/step - loss: 0.1243 - accuracy: 0.9558 - top-5-accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 58/60\n",
      "31/31 [==============================] - 6s 209ms/step - loss: 0.0404 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 59/60\n",
      "31/31 [==============================] - 7s 213ms/step - loss: 0.0156 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.2570 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 60/60\n",
      "31/31 [==============================] - 7s 212ms/step - loss: 0.0094 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.4015 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "20/20 [==============================] - 3s 143ms/step - loss: 1.0460 - accuracy: 0.7754 - top-5-accuracy: 0.9770\n",
      "Test accuracy: 77.54%\n",
      "Test top 5 accuracy: 97.7%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = create_vivit_classifier(\n",
    "    tubelet_embedder=TubeletEmbedding(\n",
    "        embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "    ),\n",
    "    positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    ")\n",
    "# Compile the model with the optimizer, loss function\n",
    "# and the metrics.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "# Train the model.\n",
    "_ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "_, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 28, 28, 28,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " tubelet_embedding_2 (TubeletEm  (None, 27, 128)     65664       ['input_3[0][0]']                \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " positional_encoder_2 (Position  (None, 27, 128)     3456        ['tubelet_embedding_2[0][0]']    \n",
      " alEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_19 (LayerN  (None, 27, 128)     256         ['positional_encoder_2[0][0]']   \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, 27, 128)     66048       ['layer_normalization_19[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 27, 128)      0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'positional_encoder_2[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 27, 128)     256         ['add_17[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_8 (Sequential)      (None, 27, 128)      131712      ['layer_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 27, 128)      0           ['sequential_8[0][0]',           \n",
      "                                                                  'add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 27, 128)     256         ['add_18[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 27, 128)     66048       ['layer_normalization_21[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 27, 128)      0           ['multi_head_attention_10[0][0]',\n",
      "                                                                  'add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_22 (LayerN  (None, 27, 128)     256         ['add_19[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, 27, 128)      131712      ['layer_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 27, 128)      0           ['sequential_9[0][0]',           \n",
      "                                                                  'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_23 (LayerN  (None, 27, 128)     256         ['add_20[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_11 (Multi  (None, 27, 128)     66048       ['layer_normalization_23[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 27, 128)      0           ['multi_head_attention_11[0][0]',\n",
      "                                                                  'add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_24 (LayerN  (None, 27, 128)     256         ['add_21[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_10 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 27, 128)      0           ['sequential_10[0][0]',          \n",
      "                                                                  'add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_25 (LayerN  (None, 27, 128)     256         ['add_22[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_12 (Multi  (None, 27, 128)     66048       ['layer_normalization_25[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 27, 128)      0           ['multi_head_attention_12[0][0]',\n",
      "                                                                  'add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 27, 128)     256         ['add_23[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_11 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 27, 128)      0           ['sequential_11[0][0]',          \n",
      "                                                                  'add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_27 (LayerN  (None, 27, 128)     256         ['add_24[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_13 (Multi  (None, 27, 128)     66048       ['layer_normalization_27[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 27, 128)      0           ['multi_head_attention_13[0][0]',\n",
      "                                                                  'add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_28 (LayerN  (None, 27, 128)     256         ['add_25[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_12 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 27, 128)      0           ['sequential_12[0][0]',          \n",
      "                                                                  'add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 27, 128)     256         ['add_26[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_14 (Multi  (None, 27, 128)     66048       ['layer_normalization_29[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 27, 128)      0           ['multi_head_attention_14[0][0]',\n",
      "                                                                  'add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 27, 128)     256         ['add_27[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_13 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 27, 128)      0           ['sequential_13[0][0]',          \n",
      "                                                                  'add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_31 (LayerN  (None, 27, 128)     256         ['add_28[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_15 (Multi  (None, 27, 128)     66048       ['layer_normalization_31[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 27, 128)      0           ['multi_head_attention_15[0][0]',\n",
      "                                                                  'add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, 27, 128)     256         ['add_29[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_14 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 27, 128)      0           ['sequential_14[0][0]',          \n",
      "                                                                  'add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, 27, 128)     256         ['add_30[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_16 (Multi  (None, 27, 128)     66048       ['layer_normalization_33[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 27, 128)      0           ['multi_head_attention_16[0][0]',\n",
      "                                                                  'add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 27, 128)     256         ['add_31[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_15 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 27, 128)      0           ['sequential_15[0][0]',          \n",
      "                                                                  'add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 27, 128)     256         ['add_32[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 128)         0           ['layer_normalization_35[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 11)           1419        ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,656,971\n",
      "Trainable params: 1,656,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import os\n",
    "import io\n",
    "import imageio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4553e14c649f4f8eb1e3f0f71719f23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(VBox(children=(HTML(value=\"'T: pancreas | P: bladder'\"), Box(children=(Image(value=b'GIF89a\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction on Test Set and Visualization:\n",
    "NUM_SAMPLES_VIZ = 25\n",
    "testsamples, labels = next(iter(testloader))\n",
    "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
    "\n",
    "ground_truths = []\n",
    "preds = []\n",
    "videos = []\n",
    "\n",
    "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
    "    # Generate gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, (testsample.numpy() * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
    "        videos.append(gif.getvalue())\n",
    "\n",
    "    # Get model prediction\n",
    "    output = model.predict(tf.expand_dims(testsample, axis=0))[0]\n",
    "    pred = np.argmax(output, axis=0)\n",
    "\n",
    "    ground_truths.append(label.numpy().astype(\"int\"))\n",
    "    preds.append(pred)\n",
    "\n",
    "\n",
    "def make_box_for_grid(image_widget, fit):\n",
    "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
    "    \"\"\"\n",
    "    # Make the caption\n",
    "    if fit is not None:\n",
    "        fit_str = \"'{}'\".format(fit)\n",
    "    else:\n",
    "        fit_str = str(fit)\n",
    "\n",
    "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
    "\n",
    "    # Make the green box with the image widget inside it\n",
    "    boxb = ipywidgets.widgets.Box()\n",
    "    boxb.children = [image_widget]\n",
    "\n",
    "    # Compose into a vertical box\n",
    "    vb = ipywidgets.widgets.VBox()\n",
    "    vb.layout.align_items = \"center\"\n",
    "    vb.children = [h, boxb]\n",
    "    return vb\n",
    "\n",
    "\n",
    "boxes = []\n",
    "for i in range(NUM_SAMPLES_VIZ):\n",
    "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
    "    true_class = info[\"label\"][str(ground_truths[i])]\n",
    "    pred_class = info[\"label\"][str(preds[i])]\n",
    "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
    "\n",
    "    boxes.append(make_box_for_grid(ib, caption))\n",
    "\n",
    "ipywidgets.widgets.GridBox(\n",
    "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_Dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4da46fc40f1915703b38afd3791e6a316bca5498ac50c4dc99c37cd966af4760"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
